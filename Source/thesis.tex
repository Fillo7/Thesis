% !TeX spellcheck = en_US

\documentclass
[
    digital, %% digital / printed
    oneside, %% oneside / twoside
    table, %% table / notable - coloring of tables
    nolof, %% lof / nolof - list of figures inclusion
    nolot, %% lot / nolot - list of tables inclusion
    nocover %% cover / nocover - cover page inclusion
    %% More options at <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>
]{fithesis3}

\usepackage[resetfonts]{cmap}
\usepackage[T1]{fontenc}
\usepackage[main=english, slovak, czech]{babel}

\thesissetup
{
    date = \the\year/\the\month/\the\day,
    university = mu,
    faculty = fi,
    type = mgr,
    author = Bc. Filip Petrovič,
    gender = m,
    advisor = {RNDr. Jiří Filipovič, Ph.D.},
    title = {Framework for Parallel Kernels Autotuning},
    TeXtitle = {Framework for Parallel Kernels Autotuning},
    keywords = {autotuning, GPU programming, OpenCL, CUDA, kernel, optimization},
    TeXkeywords = {autotuning, GPU programming, OpenCL, CUDA, kernel, optimization},
    assignment = {}
}

\thesislong{abstract}
{
    The result of this thesis is a framework for autotuning of parallel kernels which are written in either OpenCL or CUDA language. The framework
    includes advanced functionality such as support for composite kernels and online autotuning. The thesis describes API and internal structure of
    the framework and presents several examples of its utilization for kernel optimization.
}

\thesislong{thanks}
{
    I would like to thank my supervisor Jiří Filipovič for his help and valuable advice, David Střelák and Jana Pazúriková for their feedback and work
    on code examples. I would also like to thank my family for their support during my work on the thesis.
}

%% Insert bibliography
\usepackage{csquotes}
\usepackage
[
    backend=biber,
    style=numeric,
    citestyle=numeric-comp,
    sorting=none,
    sortlocale=auto
    %% More information at <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>
]{biblatex}
\addbibresource{references.bib}
\nocite{*}

%% Specify bibliography date format
\DeclareFieldFormat{urldate}
{
    (visited on
    \thefield{urlday}\addslash
    \thefield{urlmonth}\addslash
    \thefield{urlyear}\isdot)
}

%% Index generation
\usepackage{makeidx}
\makeindex

%% Allow extra table formatting commands
\usepackage{array}
\usepackage{ragged2e}
\newcolumntype{L}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\RaggedLeft\hspace{0pt}}p{#1}}

%% Enable inclusion of code samples with coloring
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset
{
    frame=tb,
    language=C,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

%% Equations and figures use same numbering
\makeatletter
\let\c@equation\c@figure
\makeatother

%% Make "thanks" section show on top of a page
\makeatletter\thesis@load
\makeatletter
    \def\thesis@blocks@thanks{%
    \ifx\thesis@thanks\undefined\else
    \thesis@blocks@clear
    \begin{alwayssingle}%
    \chapter*{\thesis@@{thanksTitle}}%
    \thesis@thanks
    \end{alwayssingle}%
    \fi}
\makeatother

\begin{document}
\chapter{Introduction}
In recent years, acceleration of complex computations using multi-core processors, graphics cards and other types of accelerators has become
much more common. Currently, there are many devices developed by multiple vendors which differ in hardware architecture, performance and other
attributes. In order to ensure portability of code written for particular device, several software APIs (application programming interfaces) such as
OpenCL (Open Computing Language) or CUDA (Compute Unified Device Architecture) were designed. Code written in these APIs can be run on various
devices while producing the same result. However, there is a problem with portability of performance due to different hardware characteristics of
these devices. For example, code which was optimized for a GPU may run poorly on a regular multi-core processor. The problem may also exist among
devices developed by the same vendor, even if they have comparable parameters and theoretical performance.

A costly solution to this problem is to manually optimize code for each utilized device. This has several significant disadvantages, such as
a necessity to dedicate large amount of resources to write different versions of code and test which one performs best on a given device. Furthermore,
new devices are released frequently and in order to efficiently utilize their capabilities, it is often necessary to rewrite old versions of code and
repeat the optimization process again.

An alternative solution is a technique called autotuning, where code includes parameters which affect performance depending on their value, for
example a parameter which affects length of a vector type of a particular variable. Optimal values of these parameters might differ for various
devices based on their hardware capabilities. Parametrized code is then launched repeatedly using different combinations of parameters to find out the
best configuration for a particular device.

In order to make autotuning easier to implement in applications, several frameworks were created. However, large number of these are focused
only on a very small subset of computations. There are some frameworks which are more general, but their features are limited and usually only support
simple usage scenarios. The aim of this thesis was to develop autotuning framework which would support more complex use cases, such as situations
where computation is split into several smaller programs. Additionally, the framework should be written in a way which would allow its easy
integration into existing software and possibly combine autotuning with regular computation.

The thesis is split into four main chapters. <Todo: add short description of each chapter.>

\chapter{Compute APIs and autotuning}
This chapter serves as an introduction to autotuning technique and includes description of compute APIs which are utilized by KTT (Kernel Tuning
Toolkit)\footnote{Name of autotuning framework developed as a part of the thesis.} -- OpenCL and CUDA. Because both APIs provide relatively similar
functionality, only OpenCL is described here in greater detail. Section about CUDA is mostly focused on explaining features which differ from OpenCL.
It is worth mentioning that CUDA actually consists of two different APIs -- low-level driver API and high-level runtime API built on top of the driver
API. For the purpose of this thesis, only CUDA driver API will be further described, because the runtime API lacks features which are necessary to
implement autotuning in CUDA.

\section{OpenCL}
OpenCL is an API for developing primarily parallel applications which can be run on a range of different devices such as CPUs, GPUs and FPGAs
(field-programmable gate arrays). An OpenCL application consists of two main parts. First part is a host program, which is typically executed on a CPU
and is responsible for OpenCL device configuration, memory management and launching of kernels. Second part is a kernel, which is a function executed
on an OpenCL device and usually contains major part of a computation. Kernels are written in OpenCL C which is based on C programming language.

\subsection{Host program in OpenCL}
Host program is written in a regular programming language, typically in C or C++. Its main objective is to successfully launch a kernel function.
OpenCL API defines several important structures which are referenced from host program:
\begin{itemize}
    \item \textit{cl\_platform} -- References an OpenCL platform.
    \item \textit{cl\_device} -- References an OpenCL device, which is used during context initialization.
    \item \textit{cl\_context} -- Serves as a holder of resources, similar in functionality to an operating system process. Majority of other OpenCL
    structures have to be tied to a specific context. Context is created for one or more OpenCL devices.
    \item \textit{cl\_command\_queue} -- All commands which are executed directly on an OpenCL device have to be submitted inside a command queue. It
    is possible to initialize multiple command queues within a single context in order to overlap independent asynchronous operations.
    \item \textit{cl\_buffer} -- Data which is directly accessed by kernel has to be bound to an OpenCL buffer, this includes both scalar and vector
    arguments. It is possible to specify buffer memory location (device or host memory) and access type (read-only, read-write, write-only).
    \item \textit{cl\_program} -- A variable which references OpenCL program compiled from OpenCL C source file. Program can be shared by multiple
    kernel objects.
    \item \textit{cl\_kernel} -- An object used to reference a specific kernel. Consists of an OpenCL program, kernel function name (single program
    can contain definitions of multiple kernel functions) and buffers which are utilized by the kernel.
    \item \textit{cl\_event} -- Serves as a synchronization primitive for individual commands submitted to an OpenCL device. Can be used to retrieve
    information about the corresponding command, such as status or execution duration.
\end{itemize}
Execution of an entire OpenCL application then typically consists of the following steps:
\begin{itemize}
    \item selection of target platform (e.g., AMD, Intel, Nvidia) and device (e.g., Intel Core i5-4690, Nvidia GeForce GTX 970)
    \item initialization of OpenCL context and one or more command queues
    \item initialization of OpenCL buffers (either in host or dedicated device memory)
    \item compilation and execution of kernel function
    \item retrieval of data produced by kernel from OpenCL buffers into host memory (if data is located in dedicated device memory)
\end{itemize}

\subsection{Kernel in OpenCL}
Code in a kernel source file is written from perspective of a single \textit{work-item}, which is the smallest OpenCL execution unit. Each work-item
has its own \textit{private memory} (memory which is mapped to e.g., CPU or GPU register).

Work-items are organized into a larger structure called \textit{work-group}, from which they all have access to \textit{local memory} (eg. CPU cache,
GPU shared memory). Work-group is executed on a single \textit{compute unit} (e.g., CPU core, GPU streaming multiprocessor). It is possible for
multiple work-groups to be executed on the same compute unit. OpenCL work-group can have up to three dimensions. Number and size of dimensions affects
work-item indexing inside work-group.

Individual work-groups are organized into \textit{NDRange} (N-Dimensional Range). At NDRange level, it is possible to address two types of memory --
\textit{global memory} and \textit{constant memory}. Global memory (e.g., CPU main memory, GPU global memory) is usually very large but has high
latency. On the other hand, constant memory generally has small capacity but lower latency. It can be utilized to store read-only data. Organization
and indexing of work-groups inside NDRange works in the same way as for work-items inside work-group.

Hierarchical organization into NDRange, work-groups and work-items allows for more intuitive mapping of computation tasks onto OpenCL kernels. Tasks
are defined at the NDrange level, work-groups represent large chunks of a task which are executed in arbitrary order. The smallest operations (e.g.
addition of two numbers) are mapped onto work-items.

Figure \ref{vector_addition} contains a simple OpenCL kernel, which adds up elements from arrays \textit{a} and \textit{b}, then stores the result in
array \textit{c}. Qualifier \textit{\_\_global} specified for the arguments means that they are stored in global memory. Function
\textit{get\_global\_id(int)} is used to retrieve work-item index unique for the entire NDRange in specified dimension.
\begin{figure}
    \begin{lstlisting}
    __kernel void vectorAddition(__global float* a, __global float* b, __global float* c)
    {
        int i = get_global_id(0);
        c[i] = a[i] + b[i];
    }
    \end{lstlisting}
    \caption{Vector addition in OpenCL.}
    \label{vector_addition}
\end{figure}

\section{CUDA, comparison with OpenCL}
CUDA is a parallel compute API developed by Nvidia Corporation. It works similarly to OpenCL, but there are also several differences which played an
important role during the framework development:
\begin{itemize}
    \item CUDA is officially supported only on graphics cards released by Nvidia Corporation
    \item Differences in terminology, same concepts have different terms in OpenCL and CUDA
    \item Global indexing (i.e., NDrange indexing in OpenCL) works differently in CUDA
\end{itemize}
The following table contains terms used in OpenCL and their counterparts in CUDA.

Todo...

\section{Autotuning in compute APIs}
Todo...

\chapter{Conclusion}
Todo...

%% Print full bibliography, use biber.exe on .bcf file to generate bibliography
\csname captions\languagename\endcsname
\makeatletter
\thesis@selectLocale{\thesis@locale}\makeatother
\printbibliography[heading=bibintoc]

\appendix
\chapter{Appendix}
Todo...

\end{document}
