% !TeX spellcheck = en_US

\documentclass
[
    digital, %% digital / printed
    oneside, %% oneside / twoside
    table, %% table / notable - coloring of tables
    nolof, %% lof / nolof - list of figures inclusion
    nolot, %% lot / nolot - list of tables inclusion
    nocover %% cover / nocover - cover page inclusion
    %% More options at <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>
]{fithesis3}

\usepackage[resetfonts]{cmap}
\usepackage[T1]{fontenc}
\usepackage[main=english, slovak, czech]{babel}
\usepackage{amssymb}
\usepackage{chngpage}

\thesissetup
{
    date = \the\year/\the\month/\the\day,
    university = mu,
    faculty = fi,
    type = mgr,
    author = Bc. Filip Petrovič,
    gender = m,
    advisor = {RNDr. Jiří Filipovič, Ph.D.},
    title = {Framework for Parallel Kernels Auto-tuning},
    TeXtitle = {Framework for Parallel Kernels Auto-tuning},
    keywords = {auto-tuning, parallel programming, OpenCL, CUDA, kernel, optimization},
    TeXkeywords = {auto-tuning, parallel programming, OpenCL, CUDA, kernel, optimization},
    assignment = {}
}

\thesislong{abstract}
{
    The result of this thesis is a framework for auto-tuning of parallel kernels which are written in either OpenCL or CUDA language. The framework
    includes advanced functionality such as support for composite kernels and online auto-tuning. The thesis describes API and internal structure of
    the framework and presents several examples of its utilization for kernel optimization.
}

\thesislong{thanks}
{
    I would like to thank my supervisor Jiří Filipovič for his help and valuable advice, David Střelák and Jana Pazúriková for their feedback and work
    on code examples. I would also like to thank my family for their support during my work on the thesis.
}

%% Insert bibliography
\usepackage{csquotes}
\usepackage
[
    backend=biber,
    style=numeric,
    citestyle=numeric-comp,
    sorting=none,
    sortlocale=auto
    %% More information at <http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf>
]{biblatex}
\addbibresource{references.bib}
\nocite{*}

%% Specify bibliography date format
\DeclareFieldFormat{urldate}
{
    (visited on
    \thefield{urlday}\addslash
    \thefield{urlmonth}\addslash
    \thefield{urlyear}\isdot)
}

%% Index generation
\usepackage{makeidx}
\makeindex

%% Allow extra table formatting commands
\usepackage{array}
\usepackage{ragged2e}
\newcolumntype{L}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\RaggedLeft\hspace{0pt}}p{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

%% Enable inclusion of code samples with coloring
\usepackage{listings}
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.6, 0.0}
\definecolor{darkred}{rgb}{0.6, 0.0, 0.0}
\definecolor{lightblue}{rgb}{0.17, 0.57, 0.69}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}

\lstset
{
    frame=tb,
    language=C++,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\footnotesize\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{darkgreen},
    stringstyle=\color{darkred},
    breaklines=true,
    breakatwhitespace=true
}

\lstset
{
    emph=[1]{float4, define, elif, endif, def, with, as, in},
    emphstyle=[1]{\color{blue}},
    emph=[2]{size\_t, uint32\_t, PlatformIndex, DeviceIndex, ComputeAPI, KernelId, ArgumentId, string, vector, DimensionVector, unique\_ptr,
        TuningManipulator, ArgumentAccessType, ArgumentMemoryLocation, ModifierType, ModifierAction, ModifierDimension, ParameterPair,
        OutputDescriptor, SearchMethod, function, ValidationMethod, ReferenceClass, PrintFormat, ostream, PlatformInfo, DeviceInfo, GlobalSizeType},
    emphstyle=[2]{\color{lightblue}}
}

%% Equations and figures use same numbering
\makeatletter
\let\c@equation\c@figure
\makeatother

%% Make "thanks" section show on top of a page
\makeatletter\thesis@load
\makeatletter
    \def\thesis@blocks@thanks{%
    \ifx\thesis@thanks\undefined\else
    \thesis@blocks@clear
    \begin{alwayssingle}%
    \chapter*{\thesis@@{thanksTitle}}%
    \thesis@thanks
    \end{alwayssingle}%
    \fi}
\makeatother

\begin{document}
\chapter{Introduction}
In recent years, acceleration of complex computations using multi-core processors, graphics cards and other types of accelerators has become
much more common. Currently, there are many devices developed by multiple vendors which differ in hardware architecture, performance and other
attributes. In order to support application development for these devices, several software APIs (application programming interfaces) such as
OpenCL (Open Computing Language) or CUDA (Compute Unified Device Architecture) were designed. Code written in these APIs can be run on various
devices while producing the same result. However, there is a problem with portability of performance due to different hardware characteristics of
these devices. For example, code which was optimized for a GPU may run poorly on a regular multi-core processor. The problem may also exist among
different generations of devices developed by the same vendor, even if they have comparable parameters and theoretical performance.

A costly solution to this problem is to manually optimize code for each utilized device. This has several significant disadvantages, such as
a necessity to dedicate large amount of resources to write different versions of code and test which one performs best on a given device. Furthermore,
new devices are released frequently and in order to efficiently utilize their capabilities, it is often necessary to rewrite old versions of code and
repeat the optimization process again.

An alternative solution is a technique called auto-tuning where a system, which supports this technique, is capable of optimizing its running parameters
in order to perform its task more efficiently. Auto-tuning is a general technique with broad range of applications, which include areas such as network
protocols, compilers and database systems. This thesis focuses on a specific form of auto-tuning called code variant auto-tuning and its application on
programs written in OpenCL and CUDA API. In this version of auto-tuning, program code contains parameters which, depending on their value, affect
performance of computation on a particular device. For example, there might be a parameter which controls length of a vector type of some variable.
Optimal values of these parameters might differ for various devices based on their hardware capabilities. Parametrized code is then launched repeatedly
using different combinations of parameters in order to find the best configuration for a particular device empirically.

To make the code variant auto-tuning process easier to implement in previously mentioned APIs, several frameworks were created. However, large number of
these are focused on domain-specific computations. There are some frameworks which are more general, but their features are limited and usually only
support simpler usage scenarios. The aim of this thesis was to develop an auto-tuning framework which would support more complex use cases, such as
situations where computation is split into several smaller functions. Additionally, the framework should be written in a way which would allow its easy
integration into existing software and possibly combine auto-tuning with regular computation.

Apart from introduction and conclusion, the thesis is split into five main chapters. Chapter one provides description of two compute APIs supported
by the new framework. It also includes possible areas of auto-tuning utilization in these APIs. Second chapter serves as an introduction to auto-tuning,
presents several existing auto-tuning frameworks and compares their strengths and weaknesses.

The following two chapters are dedicated to KTT (Kernel Tuning Toolkit) framework, which was developed in this thesis. The former provides motivation for
development of a new framework and focuses on describing its public API. The latter includes an overview of its internal structure. The fifth and final
chapter presents several scenarios of the new framework's utilization.

\chapter{Compute APIs and possibilities for auto-tuning}
This chapter includes description of compute APIs which are utilized by KTT framework -- OpenCL and CUDA. Because both APIs provide relatively similar
functionality, only OpenCL is described here in greater detail. Section about CUDA is mostly focused on explaining features which differ from OpenCL.
It is worth mentioning that CUDA actually consists of two different APIs -- low-level driver API and high-level runtime API built on top of the driver
API. This thesis includes description of the driver API only, because the runtime API lacks features which are necessary to implement auto-tuning in CUDA.

The final section of this chapter provides a list of auto-tuning opportunities in these APIs.

\section{OpenCL}
OpenCL is an API for developing primarily parallel applications which can be run on a range of different devices such as CPUs, GPUs and certain types of
accelerators. It is developed by Khronos Group, which is a consortium of several independent companies. OpenCL is therefore designed to support
hardware devices from multiple vendors. An OpenCL application consists of two main parts. First part is a host program, which is typically executed on
a CPU and is responsible for OpenCL device configuration, memory management and launching of kernels. Second part is a kernel, which is a function
executed on an OpenCL device and usually contains computationally intensive part of a program. Kernels are written in OpenCL C which is based on C
programming language.

\subsection{Host program in OpenCL}
Host program is written in a standard programming language, for example C or C++. It can handle regular inexpensive tasks such as data preparation,
input processing, network communication and others. In relation to OpenCL, its objectives include configuration of kernels, their launch,
synchronization and retrieval of results. OpenCL API defines several important structures which can be utilized to fulfill this goal:
\begin{itemize}
    \item \textit{cl\_platform} -- References an OpenCL platform.
    \item \textit{cl\_device} -- References an OpenCL device, which is used during context initialization.
    \item \textit{cl\_context} -- Serves as a holder of resources, similar in functionality to an operating system process. Majority of other OpenCL
    structures have to be tied to a specific context. Context is created for one or more OpenCL devices.
    \item \textit{cl\_command\_queue} -- All commands which are executed directly on an OpenCL device have to be submitted inside a command queue. It
    is possible to initialize multiple command queues within a single context in order to overlap independent asynchronous operations.
    \item \textit{cl\_mem} -- Data which is directly accessed by kernel has to be bound to an OpenCL buffer, this includes both scalar and vector
    arguments. It is possible to specify buffer memory location (device or host memory) and access type (read-only, read-write, write-only).
    \item \textit{cl\_program} -- A variable which references OpenCL program compiled from OpenCL C source file. Program can be shared by multiple
    kernel objects.
    \item \textit{cl\_kernel} -- An object used to reference a specific kernel. Holds information about OpenCL program, kernel function name (single
    program can contain definitions of multiple kernel functions) and buffers which are utilized by kernel.
    \item \textit{cl\_event} -- Serves as a synchronization primitive for individual commands submitted to an OpenCL device. It can be used to retrieve
    information about the corresponding command, such as status or execution duration.
\end{itemize}
Execution of an OpenCL application then typically consists of the following main steps:
\begin{itemize}
    \item selection of target platform (e.g., AMD, Intel, Nvidia) and device (e.g., Intel Core i5-4690, Nvidia GeForce GTX 970)
    \item initialization of OpenCL context and one or more command queues
    \item initialization of OpenCL buffers (either in host or dedicated device memory)
    \item compilation and execution of kernel function
    \item retrieval of data produced by kernel from OpenCL buffers into host memory (if data is located in dedicated device memory)
\end{itemize}

\subsection{Kernel in OpenCL}
\label{kernel}
Code in a kernel source file is written from a perspective of single \textit{work-item}, which is the smallest OpenCL execution unit. Each work-item
has its own \textit{private memory} (memory which is mapped to e.g., CPU or GPU register).

Work-items are organized into a larger structure called \textit{work-group}, from which they all have access to \textit{local memory} (e.g., GPU shared
memory). Work-group is executed on a single \textit{compute unit} (e.g., CPU core, GPU streaming multiprocessor). It is possible for multiple work-groups
to be executed on the same compute unit. OpenCL work-group can have up to three dimensions. Number and size of dimensions affects work-item indexing
within work-group.

Individual work-groups are organized into \textit{NDRange} (N-Dimensional Range). At NDRange level, it is possible to address two types of memory --
\textit{global memory} and \textit{constant memory}. Global memory (e.g., CPU main memory, GPU global memory) is usually very large but has high
latency. On the other hand, constant memory generally has small capacity but lower latency. It can be utilized to store read-only data. Organization
and indexing of work-groups inside NDRange works in the same way as for work-items within work-group. The entire hierarchy is illustrated in picture
\ref{opencl-hierarchy}.

\begin{figure}
    \begin{center}
    \includegraphics[width=125mm]{resources/opencl_hierarchy.png}
    \end{center}
    \caption{OpenCL memory hierarchy. Source: \cite{opencl-hierarchy-diagram}}
    \label{opencl-hierarchy}
\end{figure}

Hierarchical organization into NDRange, work-groups and work-items allows for more flexible mapping of computation tasks onto heterogeneous hardware
devices, which can have different architectures. Furthermore, it may also make it easier to map tasks onto OpenCL kernels. Complete tasks are defined
at the NDRange level, work-groups represent large computation chunks which are executed in arbitrary order. The smallest operations (e.g., addition of
two numbers) are mapped onto work-items.

Figure \ref{vector_addition} contains a simple OpenCL kernel, which performs addition of elements from arrays \textit{a} and \textit{b}, then stores
the result in array \textit{c}. Qualifier \textit{\_\_global} specified for the arguments means that they are stored in global memory. Function
\textit{get\_global\_id(int)} is used to retrieve work-item index unique for the entire NDRange in specified dimension.
\begin{figure}
\begin{lstlisting}
__kernel void vectorAddition(__global float* a, __global float* b, __global float* c)
{
    int i = get_global_id(0);
    c[i] = a[i] + b[i];
}
\end{lstlisting}
\caption{Vector addition in OpenCL.}
\label{vector_addition}
\end{figure}

\section{CUDA, comparison with OpenCL}
CUDA is a parallel compute API developed by Nvidia Corporation. It works similarly to OpenCL, but there are also several differences which played an
important role during the framework development:
\begin{itemize}
    \item CUDA is officially available only for graphics cards released by Nvidia Corporation and CPUs. GPUs developed by other vendors and other types
    of accelerators are not supported.
    \item Differences in terminology -- identical or similar concepts have different terms in OpenCL and CUDA.
    \item Global indexing (i.e., NDRange indexing in OpenCL) works differently in CUDA.
\end{itemize}
Table \ref{api_terms} contains terms used in OpenCL and their counterparts in CUDA. Due to several differences in design, some terms do not have an
equivalent term in the other API.
\begin{table}[h]
\begin{center}
\begin{tabular}{|*{2}{p{5cm}|}}
    \hline
    OpenCL term & CUDA term \\
    \toprule
    compute unit & streaming multiprocessor \\
    NDRange & grid \\
    work-group & thread block \\
    work-item & thread \\
    global memory & global memory \\
    constant memory & constant memory \\
    local memory & shared memory \\
    private memory & local memory \\
    cl\_platform & N/A \\
    cl\_device\_id & CUdevice \\
    cl\_context & CUcontext \\
    cl\_command\_queue & CUstream \\
    cl\_mem & CUdeviceptr \\
    cl\_program & nvrtcProgram, CUmodule \\
    cl\_kernel & CUfunction \\
    cl\_event & CUevent \\
    \hline
\end{tabular}
\end{center}
\caption{Comparison between OpenCL and CUDA terminology.}
\label{api_terms}
\end{table}

Difference in global indexing plays an important role during addition of tuning parameters which affect either grid dimensions or block dimensions.
In OpenCL, the NDRange size is specified as total number of work-items in a dimension. However, in CUDA the grid size is specified as number of threads
in a dimension divided by number of blocks in that dimension. This is rather inconvenient for porting auto-tuned programs from one API to the other.
The problem will be further elaborated upon in chapter \ref{ktt-api}.

\section{Possibilities for auto-tuning in compute APIs}
Design of previously described APIs allows for a wide range of optimization opportunities, both inside kernel and host code. These optimizations can
be implemented with usage of tuning parameters. While some of the parameters can be utilized only in a limited range of applications, there are also
several ones which are relevant for larger number of computation tasks. This section provides a list of some of the most common optimization parameters
which are used in auto-tuning.

\subsection{Work-group (thread block) dimensions}
Work-group dimensions specify how many work-items are included in a single work-group. Work-groups are executed on compute units, which are mapped
onto, for example CPU cores or GPU multiprocessors. Performance of these devices may be vastly different and manually finding an optimal work-group
size is difficult. The dimensions also indirectly affect cache locality of data, which is a reason why this parameter usually makes an ideal candidate
for auto-tuning.

\subsection{Usage of vector data types}
Modern processors contain vector registers that allow concurrent execution of a single instruction over multiple data which leads to a significant
speed-up of certain types of computations. Kernel compilers attempt to automatically utilize these registers in order to speed up computation without
manual code modification. However, automatic vectorization is not always optimal. There is an option to perform manual vectorization by using vector
data types which are available in both OpenCL and CUDA. It is possible to control vector length with a tuning parameter, e.g., by using type aliases.

\subsection{Data placement in different types of memory}
Section \ref{kernel} described various memory types available in OpenCL, similar memory hierarchy can also be found in CUDA. In many cases, there are
more valid memory types to choose from for data placement. The choice can have an effect on performance, for example accessing data from OpenCL local
memory is usually faster than using global memory. The problem is that local memory capacity is limited and while on certain devices the data could
fit into it, on other devices it would be necessary to use global memory instead. Having a single version of kernel which would utilize only global
memory would be inefficient for large number of devices. This can be solved by using tuning parameter which controls the data memory placement.

\subsection{Data layout in memory}
Composite data can be organized into memory in multiple ways. For example, data about 3D vertex coordinates can be split into three separate arrays
which are then stored in memory one by one. Another way to organize the same data is to first put all information about vertex into a structure and
then create an array of these structures. The former layout is commonly referred to as structure of arrays (SoA), while the latter is called array
of structures (AoS). The difference is illustrated on figure \ref{aos_soa}.

The benefit of SoA is that variables with same data type are stored in contiguous memory, which enables certain devices (e.g., Intel CPUs) to more
efficiently utilize vector instructions. Other types of devices such as GPUs support native vector addressing and usage of SoA layout may lead to
performance degradation.

\begin{figure}
\begin{lstlisting}
// 3D vertex coordinates stored as AoS and SoA.
struct                       struct
{                            {
    int x;                       int x[N];
    int y;                       int y[N];
    int z;                       int z[N];
} AoS[N];                    } SoA;
\end{lstlisting}
\caption{Comparison of array of structures and structure of arrays layouts.}
\label{aos_soa}
\end{figure}

\chapter{Code variant auto-tuning and related frameworks}
This chapter describes common terms used in relation to auto-tuning and provides a list of desired features which should be supported by auto-tuning
frameworks. Afterwards, several generic frameworks are presented, including their advantages, disadvantages, usage examples and comparison. Frameworks
which are domain-specific or not publicly available are not discussed here.

\section{Auto-tuning glossary}
The following terms are commonly encountered in subsequent chapters and their knowledge is required for better understanding of auto-tuning process:
\begin{itemize}
    \item \textit{tuning parameter} -- Parameter which, depending on its value, affects performance of a computation. For example, a parameter which
    controls length of a vector type of some variable. The exact way parameter comes into effect depends on a specific framework. Common option is
    utilization of just-in-time compilation and preprocessor macros.
    \item \textit{configuration space} -- Space which is created as a Cartesian product of all tuning parameters and their values.
    \item \textit{tuning configuration} -- Single element of configuration space.
    \item \textit{traversal of configuration space} -- A process where tuned program is launched repeatedly with different tuning configurations and
    its running time is measured.
    \item \textit{search method} -- A method employed to explore individual tuning configurations. Because configuration space may become very large,
    exhaustive search is not always a viable option. It is possible to explore the space randomly or utilize heuristics.
\end{itemize}

\section{Features of auto-tuning frameworks}
While there are no strict requirements over functionality that should be available in auto-tuning frameworks, there are several features which are either
commonly implemented by existing frameworks or desired by users. They include the following: 
\begin{itemize}
    \item Tuning parameters -- While this feature is supported by essentially all frameworks, not all of them allow parameters to affect
    all parts of a program. For example, some frameworks support only parameters which affect kernel code but not host code.
    \item Parameter constraints -- Ability to mark certain tuning configurations as invalid due to incompatible combinations of tuning parameter values.
    Such configurations should be excluded from configuration space traversal, which can lead to improved usability and performance.
    \item Search methods -- Support for different methods which offer reasonable performance and quality of results.
    \item Output validation -- Certain tuning configurations might include code which is experimental or still in development. Tuner should offer
    an ability to compare the produced output with precomputed reference output and detect differences.
    \item Usage of kernel compositions -- A computation may utilize multiple kernels in order to produce complete result. For example, output produced
    by kernel for matrix transposition is then used as an input for matrix multiplication kernel. The two kernels may each have different tuning
    parameters and such scenarios require extra functionality on framework side.
    \item Online auto-tuning -- Ability to combine configuration space traversal with regular computation. Output from tested tuning configurations can
    be immediately utilized in other parts of a program.
    \item Integration into existing software -- Possibility to optionally modify certain auto-tuning steps, such as launching of individual kernels in
    order to allow execution of arbitrary code.
    \item User-friendliness and ease of use -- Availability of documentation, tutorials, clean and stable API. Availability of utility methods such
    as printing of tuning results in common format, logging of debug information and others.
\end{itemize}

\section{CLTune}
CLTune \cite{cltune} is a framework for auto-tuning of OpenCL and CUDA kernels. It is freely available in form of a library and provides C++ interface
for writing host programs. It is relatively easy to use and provides capabilities for tuning of single kernels, multiple configuration search
strategies including several heuristic-based approaches and result validation in a form of reference kernels.

However, it also has several limitations. Among the most significant ones are lack of support for kernel compositions, limited argument handling
options (all kernels must accept same kernel arguments, argument placement in memory is impossible to control) and poor support for integration into
existing software (code which launches kernels is internal part of the framework and cannot be modified). The framework is no longer actively developed,
so it is unlikely that new features will be introduced.

Basic tuner configuration in CLTune consists of several main steps, which are listed below. KTT functionality, in its simplest form, is based on the
same idea. Figure \ref{cltune-example} contains part of a program written in CLTune, which includes all of the following steps:
\begin{enumerate}
    \item Initialization of tuner by specifying target platform and device.
    \item Addition of tuned kernel.
    \item Addition of reference kernel for output validation.
    \item Definition of tuning parameters.
    \item Setup of kernel arguments.
    \item Launch of the tuning process.
    \item Retrieval of results.
\end{enumerate}
\begin{figure}
\begin{lstlisting}
cltune::Tuner tuner(platformIndex, deviceIndex);
size_t kernelId = tuner.AddKernel({"path/to/kernel.cl"}, "kernelName", ndRangeDimensions, workGroupDimensions);
tuner.SetReference({"path/to/reference_kernel.cl"}, "referenceKernelName", ndRangeDimensions, workGroupDimensions);

tuner.AddParameter(kernelId, "VECTOR_TYPE", { 1, 2, 4, 8 });
tuner.AddParameter(kernelId, "USE_CONSTANT_MEMORY", { 0, 1 });

tuner.AddArgumentInput(bufferA);
tuner.AddArgumentInput(bufferB);
tuner.AddArgumentScalar(helperVariable);
tuner.AddArgumentOutput(bufferResult);

tuner.Tune();
tuner.PrintToScreen();
\end{lstlisting}
\caption{Host program written in CLTune.}
\label{cltune-example}
\end{figure}

In order to support tuning parameters, kernel source file needs to be modified. In case of CLTune, the tuner exports parameter values from given
configuration to kernel source code by using preprocessor macros. Kernel code has to be modified by user, so that the exported values have intended
effect on computation. Simple example of such modification is shown in figure \ref{cltune-example-kernel}.
\begin{figure}
\begin{lstlisting}
#if USE_CONSTANT_MEMORY == 0
#define MEMORY_TYPE __global
#elif USE_CONSTANT_MEMORY == 1
#define MEMORY_TYPE __constant
#endif

__kernel void tunedKernel(MEMORY_TYPE float* bufferA, ...)
{
    ...
}
\end{lstlisting}
\caption{Adding support for auto-tuning to kernel via preprocessor macros.}
\label{cltune-example-kernel}
\end{figure}

\section{Kernel Tuner}
Kernel Tuner \cite{kerneltuner} is another open-source auto-tuning framework. It supports tuning of OpenCL and CUDA kernels as well as regular C functions,
though in the last case, user is responsible for measuring execution duration. API is provided for Python. Compared to CLTune, it provides more utility
methods, for example ability to set kernel compiler options, measuring execution duration in multiple iterations to increase accuracy and validating output
with user-defined precomputed answer rather than being restricted to reference kernel.

As in case of CLTune, disadvantages include lack of support for kernel compositions and inability for integration into existing software. However,
Kernel Tuner is still actively developed and some of these shortcomings may be eventually amended.

Host program has to be written in similar fashion to CLTune, definitions of tuning parameters are exported in the same way. Figure \ref{kerneltuner-example}
contains major portion of host program written for Kernel Tuner.
\begin{figure}
\begin{lstlisting}
def tune():

with open('stencil.cl', 'r') as f:
kernel_string = f.read()

problem_size = (4096, 2048)
size = numpy.prod(problem_size)

x_old = numpy.random.randn(size).astype(numpy.float32)
x_new = numpy.copy(x_old)
args = [x_new, x_old]

tune_params = OrderedDict()
tune_params["block_size_x"] = [32*i for i in range(1,9)]
tune_params["block_size_y"] = [2**i for i in range(6)]

grid_div_x = ["block_size_x"]
grid_div_y = ["block_size_y"]

return kernel_tuner.tune_kernel("stencil_kernel", kernel_string, problem_size, args, tune_params, grid_div_x=grid_div_x, grid_div_y=grid_div_y, verbose = True)

if __name__ == "__main__":
    tune()
\end{lstlisting}
\caption{Host program written in Kernel Tuner. Source: \cite{kerneltuner-example}}
\label{kerneltuner-example}
\end{figure}

\section{OpenTuner}
Unlike CLTune and Kernel Tuner, OpenTuner \cite{opentuner} is an auto-tuning framework which can be used to tune programs written in essentially any
language. API is provided for Python. Due to tuner's more generic nature, users are responsible for writing more sections of code themselves. This
involves, for example writing a method which adds parameter definitions from tuner-generated configurations to tuned program and compiles it. The
other listed frameworks have this functionality already built-in. Other shortcomings include problems with integration into software and lack of
complete API documentation. Framework no longer seems to be actively developed with majority of the development being done before 2017.

While the other frameworks use preprocessor definitions to export tuning parameters into code, OpenTuner does not have any specific way of parameter
handling. The way parameters become visible in tuned code depends on capabilities of target programming language and on the user-written method for
parameter export. Figure \ref{opentuner-example} contains an example of OpenTuner configuration for tuning of C code. Tuning parameters are added
to code through compiler command line arguments.

\begin{figure}
\begin{lstlisting}
class GccFlagsTuner(MeasurementInterface):

def manipulator(self):
manipulator = ConfigurationManipulator()
manipulator.add_parameter(IntegerParameter('vectorType', 1, 2, 4, 8))
return manipulator

def run(self, desired_result, input, limit):
cfg = desired_result.configuration.data

gcc_cmd = 'g++ tuned_program.cpp '
gcc_cmd += '-VECTOR_TYPE='+ cfg['vectorType']
gcc_cmd += ' -o ./tmp.bin'

compile_result = self.call_program(gcc_cmd)
assert compile_result['returncode'] == 0

run_cmd = './tmp.bin'
run_result = self.call_program(run_cmd)
assert run_result['returncode'] == 0

return Result(time=run_result['time'])

def save_final_config(self, configuration):
print "Optimal vector type written to final_config.json:", configuration.data
self.manipulator().save_to_file(configuration.data, 'final_config.json')
\end{lstlisting}
\caption{Configuration of OpenTuner, source: \cite{opentuner}.}
\label{opentuner-example}
\end{figure}

\section{Comparison of frameworks}
Table \ref{frameworks-comparison} showcases state of features in previously discussed frameworks.

\begin{table}[h]
\begin{adjustwidth}{-2.4cm}{0cm}
\centering
\begin{tabular}{|L{4cm}L{4cm}L{4cm}L{4cm}|}
\hline
\multicolumn{1}{|c}{\textbf{Feature}} & \multicolumn{1}{c}{\textbf{CLTune}} & \multicolumn{1}{c}{\textbf{Kernel Tuner}} & \multicolumn{1}{c|}{\textbf{OpenTuner}} \\
\hline
Supported APIs & CUDA, OpenCL & CUDA, OpenCL & any language / API \\
Tuning parameters & kernel code only & kernel and host code & kernel and host code \\
Parameter constraints & \multicolumn{1}{c}{\checkmark} & \multicolumn{1}{c}{\checkmark} & \multicolumn{1}{c|}{\checkmark} \\
Search methods & multiple heuristics supported & multiple heuristics supported & multiple heuristics supported \\
Output validation & reference kernel only & reference kernel or precomputed buffer & \multicolumn{1}{c|}{X} \\
Kernel compositions & \multicolumn{1}{c}{X} & \multicolumn{1}{c}{X} & \multicolumn{1}{c|}{X} \\
Online auto-tuning & \multicolumn{1}{c}{X} & \multicolumn{1}{c}{X} & \multicolumn{1}{c|}{X} \\
Full API documentation & \multicolumn{1}{c}{\checkmark} & \multicolumn{1}{c}{\checkmark} & \multicolumn{1}{c|}{X} \\
\hline
\end{tabular}
\caption{Comparison of features in auto-tuning frameworks.}
\label{frameworks-comparison}
\end{adjustwidth}
\end{table}

\chapter{KTT framework}
\label{ktt-api}

\section{Development of new framework}
While the previously mentioned frameworks handle auto-tuning of single kernels well and provide fairly wide range of utility methods, they all lack
support for tuning of kernel compositions and online auto-tuning. They were designed for tuning of separate programs and did not have possibility
of integration into existing software in mind. These are the main features which should be included in the new auto-tuning framework.

Originally, CLTune was planned to be used as a basis for the new framework. Extra functionality should be added on top of the existing code structure.
However, this has proved to be rather problematic. While CLTune API is written in a clean and user-friendly manner, its internal structure made it
difficult to extend its functionality. Large part of the internal code is placed into a small number of very long methods which mix together operations
such as argument handling and result validation with access to compute API functions. This made it difficult to introduce new features without refactoring
large amount of code.

Eventually, it was decided to write a completely new framework named Kernel Tuning Toolkit. Baseline portion of KTT API remained similar to CLTune,
so it would be easy to port existing programs. However, the internal structure was completely rewritten from scratch, with only very small portions
of CLTune code for following features being reused:
\begin{itemize}
    \item generating of tuning configurations
    \item definition of tuning parameter constraints
    \item search methods based on simulated annealing and particle swarm optimization techniques
\end{itemize}
The new tuner structure is further discussed in chapter \ref{ktt-structure}.

\section{KTT API}
KTT framework API provides users with methods which can be used to develop and tune OpenCL or CUDA applications. It is split into three major
classes, some basic methods were inspired by CLTune. It is available in C++ language.

KTT framework can be acquired from GitHub as a fully open-source library with prebuilt binaries being available for certain platforms. Manual library
compilation is also possible by using build tool premake5, C++14 compiler and CUDA or OpenCL distribution. Supported operating systems include Linux
and Windows.

The described API corresponds to version 0.6 of KTT framework. It is the first release candidate version and contains all functionality that was
planned to be implemented as part of this thesis.

\section{Tuner class}
Tuner class makes up the main part of KTT API. It includes methods which implement following functionality:
\begin{itemize}
    \item handling of kernels and kernel compositions
    \item handling of kernel arguments
    \item addition of tuning parameters and constraints
    \item kernel running and tuning
    \item kernel output validation
    \item retrieval of tuning results
    \item retrieval of information about available platforms and devices
\end{itemize}

\subsection{Tuner creation}
In order to access the API methods, tuner object has to be created. There are currently three versions of tuner constructors available (figure 
\ref{ktt-constructors}). They allow specification of compute API (either OpenCL or CUDA), platform index, device index and number of utilized compute
queues. OpenCL API with one compute queue is the default setting. Indices are assigned to platforms and devices by KTT framework, they can be retrieved
with a method.

\begin{figure}
\begin{lstlisting}
Tuner(const PlatformIndex, const DeviceIndex)
Tuner(const PlatformIndex, const DeviceIndex, const ComputeAPI)
Tuner(const PlatformIndex, const DeviceIndex, const ComputeAPI, const uint32_t computeQueueCount)
\end{lstlisting}
\caption{Tuner constructors.}
\label{ktt-constructors}
\end{figure}

\subsection{Kernel handling}
Kernels can be added to tuner from a file or C++ string. Users furthermore need to specify kernel function name and default global and local sizes
(i.e., dimensions for NDRange / grid and work-group / thread block). The sizes are stored inside \textit{DimensionVector} objects, which are a part of
KTT framework. They allow easy thread size manipulation and support up to three dimensions. Existing kernels can be referenced by using a handle returned
by tuner. Kernel compositions can be added by specifying handles of kernels included inside composition. In order to use compositions, user additionally
has to define a tuning manipulator class whose usage is detailed in section \ref{ktt-manipulator}.
\begin{figure}
\begin{lstlisting}
KernelId addKernel(const std::string& source, const std::string& kernelName, const DimensionVector& globalSize, const DimensionVector& localSize)
KernelId addKernelFromFile(const std::string& filePath, const std::string& kernelName, const DimensionVector& globalSize, const DimensionVector& localSize)
KernelId addComposition(const std::string& compositionName, const std::vector<KernelId>& kernelIds, std::unique_ptr<TuningManipulator>)
\end{lstlisting}
\caption{Kernel addition methods.}
\label{ktt-kernel-methods}
\end{figure}

\subsection{Kernel argument handling}
There are three types of kernel arguments supported by KTT -- vector, scalar and local memory (OpenCL) arguments. All arguments are referenced by using
a handle provided by tuner. Argument addition methods are templated and support primitive data types (e.g., int, float) as well as user-defined data
types (e.g., struct, class). Arguments are bound to kernels by using a method which accepts kernel handle and corresponding argument handles. This allows
them to be shared among multiple kernels.

Vector arguments are added from C++ vector containers. It is possible to specify access type (read, write or combined), memory location from where
argument data is accessed by kernel (device or host) and whether argument copy should be made by tuner. By default, copies of all vector arguments
are made by tuner, so the original vectors remain modifiable by user without interfering with tuning process. In case argument is placed in host
memory, it is possible to utilize zero-copy feature, which means that kernel has direct access to buffer which was initialized from host code. This
functionality is supported by both CUDA and OpenCL. All of the vector argument handling options are illustrated with diagram \ref{ktt-buffer-diagram}.

\begin{figure}
\begin{lstlisting}
ArgumentId addArgumentVector(const std::vector<T>& data, const ArgumentAccessType)
ArgumentId addArgumentVector(std::vector<T>& data, const ArgumentAccessType, const ArgumentMemoryLocation, const bool copyData)
ArgumentId addArgumentScalar(const T& data)
ArgumentId addArgumentLocal(const size_t localMemoryElementsCount)
void setKernelArguments(const KernelId, const std::vector<ArgumentId>&)
\end{lstlisting}
\caption{Argument handling methods.}
\label{ktt-argument-methods}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=125mm]{resources/ktt_buffer_diagram.png}
\end{center}
\caption{Vector argument handling options in KTT framework. Each box represents one copy of a buffer.}
\label{ktt-buffer-diagram}
\end{figure}

\subsection{Tuning parameters and constraints}
Tuning parameters are specified for kernels with a name and list of valid values. Both integer and floating-point values are supported. Before kernel
tuning begins, configurations for each combination of kernel parameter values are generated. For example, adding parameter A with values 1 and 2, and
parameter B with values 5 and 10 will result in four configurations being generated -- \{A1, B5\}, \{A1, B10\}, \{A2, B5\} and \{A2, B10\}. Tuned kernel is
then launched with parameter definitions prepended to kernel source code based on the current configuration.

Some tuning parameters may additionally affect global and local sizes of tuned kernel. This is useful, for example in cases where parameter in kernel
source code modifies amount of work done by a single work-item and therefore changes total number of needed work-items. Each dimension can be modified
separately, supported modifiers include addition, subtraction, multiplication and division.

If certain combinations of tuning parameters are invalid or unsupported by kernel source code, they can be eliminated by using parameter constraints.
Constraint is a function which accepts list of parameter values for specified parameters and returns a boolean result which signifies whether the
combination is valid or not. Constraint conditions are defined by user.

Tuning parameters for kernel compositions are added separately and are independent from kernels. Individual kernels are not affected by composition
parameters either.

\begin{figure}
\begin{lstlisting}
void addParameter(const KernelId, const std::string& parameterName, const std::vector<size_t>& parameterValues)
void addParameterDouble(const KernelId, const std::string& parameterName, const std::vector<double>& parameterValues)
void addParameter(const KernelId, const std::string& parameterName, const std::vector<size_t>& parameterValues, const ModifierType, const ModifierAction, const ModifierDimension)
void addConstraint(const KernelId, const std::function<bool(std::vector<size_t>)>& constraintFunction, const std::vector<std::string>& parameterNames)
\end{lstlisting}
\caption{Tuning parameter and constraint addition methods.}
\label{ktt-parameter-methods}
\end{figure}

\subsection{Kernel tuning and running}
KTT supports offline and online kernel tuning as well as regular kernel running. In offline tuning, kernel configurations are tested iteratively one
after another without interruption. This mode is strictly focused on finding the best performing configuration, retrieval of kernel output by user and
swapping of kernel argument data between configurations is not possible. On the other hand, it allows efficient validation of output. Because the
argument data remains the same for all configurations, the reference output needs to be computed only once.

In online tuning, single configuration is tested at time, enabling combination with kernel running. It also allows kernel output retrieval using
KTT built-in structure \textit{OutputDescriptor}. This structure specifies handle of argument to be retrieved, memory location for output data and
optionally size of the retrieved data, which is useful in case only part of the argument is needed. Online tuning also enables swapping of argument
data between each configuration, though if validation is enabled, reference output needs to be recomputed every time a new configuration is run.

In both modes, the order and number of tested configurations depends on utilized search method. KTT currently supports four search methods -- full
search, random search, simulated annealing and Markov chain Monte Carlo. Full search simply explores all configurations iteratively. The other three
methods allow specification of a fraction parameter which controls number of explored configurations (e.g., setting fraction to 0.5 will result in
50\% of all configurations being tested). In random search, the explored configurations are chosen randomly, while the last two methods employ
probabilistic techniques in order to find configurations with good performance more quickly.

Output retrieval is supported for kernel running in same fashion as for online tuning. Kernels can be run in any valid tuning configuration which is
specified by user. Output validation is not performed during kernel running.

In basic case, launch of a kernel is handled automatically by tuner after the initial setup. However, for scenarios where part of a computation
happens in C++ code or kernel compositions are utilized, it is necessary to implement a \textit{TuningManipulator} and then bind it to corresponding
kernel. Tuning manipulators are discussed in greater detail in section \ref{ktt-manipulator}.

\begin{figure}
\begin{lstlisting}
void tuneKernel(const KernelId)
void tuneKernelByStep(const KernelId, const std::vector<OutputDescriptor>& output)
void runKernel(const KernelId, const std::vector<ParameterPair>& configuration, const std::vector<OutputDescriptor>& output)
void setSearchMethod(const SearchMethod, const std::vector<double>& arguments)
void setTuningManipulator(const KernelId, std::unique_ptr<TuningManipulator>)
\end{lstlisting}
\caption{Kernel tuning and running methods.}
\label{ktt-tuning-methods}
\end{figure}

\subsection{Output validation}
Kernel output can be validated in two ways -- with a reference class or a reference kernel. In former case, user has to implement a class which
includes a method that computes reference output on a CPU. Tuner then compares this output with result produced by tuned kernels. If difference in tuned
output at certain index is detected, given kernel configuration is considered invalid. More details about reference class can be found in section
\ref{ktt-reference-class}. The latter case works similarly, difference being that reference output is computed by a kernel with user-specified
configuration. Both methods support validation of multiple kernel arguments. It is also possible to only check subpart of the argument, which is useful
when result is shorter than length of an entire argument.

When kernel arguments with floating-point data type are validated, user can choose one of the multiple validation techniques and a tolerance threshold.
If tuned output differs slightly from reference output, but remains within the threshold, it is still considered correct. Validation techniques include
side by side comparison where result difference is calculated and compared to threshold for each pair of elements with corresponding index in reference
and tuned output. Other technique is absolute difference, where the differences between individual pairs are summed up and only the resulting sum is
compared to threshold.

Users additionally have an option to add a custom comparator for specified argument. Comparator is a method which receives two elements with same data
type and decides whether they are equal. Comparators are mandatory for arguments with user-defined data types as the tuner is only able to automatically
validate arguments with built-in data types.

\begin{figure}
\begin{lstlisting}
void setReferenceKernel(const KernelId id, const KernelId referenceId, const std::vector<ParameterPair>& referenceConfiguration,
const std::vector<ArgumentId>& validatedArgumentIds)
void setReferenceClass(const KernelId, std::unique_ptr<ReferenceClass>, const std::vector<ArgumentId>& validatedArgumentIds)
void setValidationMethod(const ValidationMethod, const double toleranceThreshold)
void setValidationRange(const ArgumentId, const size_t range)
void setArgumentComparator(const ArgumentId, const std::function<bool(const void*, const void*)>& comparator)
\end{lstlisting}
\caption{Output validation methods.}
\label{ktt-validation-methods}
\end{figure}

\subsection{Tuning results retrieval}
Each tuning result includes list of parameter values, global and local thread sizes and corresponding duration of computation. List of all tuning
results for specified kernel can be printed either to a C++ output stream or a file. Supported print formats include verbose format intended for log
files or terminals and CSV (comma-separated values) format which is useful for subsequent processing and analysis of results.

List of parameter values for the best known configuration can also be retrieved programmatically through API, which is useful for combining online
auto-tuning with kernel running.

\begin{figure}
\begin{lstlisting}
void printResult(const KernelId, std::ostream& outputTarget, const PrintFormat) const
void printResult(const KernelId, const std::string& filePath, const PrintFormat) const
std::vector<ParameterPair> getBestConfiguration(const KernelId) const
\end{lstlisting}
\caption{Result retrieval methods.}
\label{ktt-result-methods}
\end{figure}

\subsection{Platforms and devices information retrieval}
When using KTT framework for the first time on a system, it is useful to retrieve indices for available platforms and devices, which are then used for
proper tuner initialization. The assigned indices and corresponding platform and device names can be printed to specified C++ output stream. It is
furthermore possible to retrieve more detailed information about individual platforms and devices, such as list of supported extensions, memory
capacities, number of compute units and others.

\begin{figure}
\begin{lstlisting}
void printComputeAPIInfo(std::ostream& outputTarget) const
std::vector<PlatformInfo> getPlatformInfo() const
std::vector<DeviceInfo> getDeviceInfo(const PlatformIndex) const
DeviceInfo getCurrentDeviceInfo() const
\end{lstlisting}
\caption{Information retrieval methods.}
\label{ktt-information-methods}
\end{figure}

\subsection{Other notable methods}
Other notable API methods include a method for specification of kernel compiler options, choice of a global size notation and an option to enable
automatic global size correction. Compiler options can be specified as a string of individual flags separated by a white space.

Choice of a global size notation allows using OpenCL NDRange dimension specification for CUDA grid and vice versa. This allows elimination of one of
the notable differences between OpenCL and CUDA API in host code and makes it easier to port programs written in one API to the other.

Automatic global size correction ensures that global size is always a multiple of local size, which is a necessary requirement for running kernels
in OpenCL, and also in CUDA if OpenCL global size notation option is used. Framework performs automatic roundup of a global size to the nearest higher
multiple of a local size. Enabling this behaviour is useful when multiple tuning parameters which affect thread sizes are present.

\begin{figure}
\begin{lstlisting}
void setCompilerOptions(const std::string& options)
void setGlobalSizeType(const GlobalSizeType)
void setAutomaticGlobalSizeCorrection(const bool flag)
\end{lstlisting}
\caption{Other notable methods.}
\label{ktt-utility-methods}
\end{figure}

\section{Reference class}
\label{ktt-reference-class}
Reference class is an interface provided by KTT framework used for validating of kernel output via implementing a C++ function. In order to utilize it,
a new class which publicly inherits from \textit{ReferenceClass} interface must be defined by a user. For the resulting class to be valid, it is
necessary to implement two virtual methods and optionally override one more method (figure \ref{ktt-reference-methods}).

First method should implement computation of a reference output. Second method is then used to retrieve the prepared output. Third, optional method can
be overriden if the resulting output size is smaller than the size of corresponding validated kernel argument. This is useful for situations where only
a part of the argument is validated.

Implemented class can be then assigned to tuner by using a method from tuner API described in section \ref{ktt-validation-methods}. The implemented
methods are utilized by tuner during output validation phase.

\begin{figure}
\begin{lstlisting}
virtual void computeResult() = 0
virtual void* getData(const ArgumentId) = 0
virtual size_t getNumberOfElements(const ArgumentId) const
\end{lstlisting}
\caption{Reference class methods.}
\label{ktt-reference-methods}
\end{figure}

\section{Tuning Manipulator class}
\label{ktt-manipulator}
Todo...

\section{Example of API usage}
Todo...

\chapter{KTT Structure}
\label{ktt-structure}
Todo...

\chapter{Advanced KTT usage examples}
\label{ktt-usage}
Todo...

\chapter{Conclusion}
Todo...

%% Print full bibliography, use biber.exe on .bcf file to generate bibliography
\csname captions\languagename\endcsname
\makeatletter
\thesis@selectLocale{\thesis@locale}\makeatother
\printbibliography[heading=bibintoc]

\appendix
\chapter{Appendix}
Todo...

\end{document}
